---
layout:     post
title:  基于Scrapy框架的IP代理的使用
subtitle: IP代理池
date:       2017-11-08
author:     QC
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - IP
---
### 基于Scrapy框架的IP代理池的使用

#### 1.前言

首先先讲一下什么叫代理，简单的说，代理就是换个身份，网络中的身份之一就是IP。比如，我们身在墙内，想要访问google，facebook等，直接访问是404，所以要换一个不会被墙的IP，比如国外的IP等，这个就是简单的代理。

在爬虫中，有些网站为了防止爬虫或者或者DDOS攻击等，会记录每个IP的访问次数。比如，有些网站允许一个IP在1S（或者别的时间）内只能访问10次等，那我们就需要访问一次换一个IP（具体什么策略，自己决定）。

下面问题来了，我们想要使用代理IP，首先是得到代理IP，那么这些代理IP从哪里来？对于公司来讲，买代理IP，对于个人来讲可能会有些浪费，因为我现在正处于调研阶段，所以我选择去免费的代理IP网站去找代理IP，但是网上很多免费的IP不可用，所以我试着找了一个脚本将免费代理IP网站上的IP先爬下来然后检测它可不可用，这样的话就方便很多。

#### 2.获取可用的免费代理IP

> 代码如下：

```
# -*- coding:utf-8 -*-
from bs4 import BeautifulSoup
import socket
import urllib
import urllib.request

User_Agent = 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0'
header = {}
header['User-Agent'] = User_Agent

# 获取所有代理IP地址
def getProxyIp():
    proxy = []
    for i in range(1,2):
        try:
            url = 'http://www.xicidaili.com/nn/'+str(i)
            req = urllib.request.Request(url,headers=header)
            res = urllib.request.urlopen(req).read()
            soup = BeautifulSoup(res,"lxml")
            ips = soup.findAll('tr')
            for x in range(1,len(ips)):
                ip = ips[x]
                tds = ip.findAll("td")
                ip_temp = tds[1].contents[0]+"\t"+tds[2].contents[0]
                proxy.append(ip_temp)
        except:
            continue
        return proxy
#验证获得的代理IP地址是否可用
def validateIp(proxy):
    url = "http://ip.chinaz.com/getip.aspx"
    f = open("E:\ip.txt","w")
    socket.setdefaulttimeout(3)
    for i in range(0,len(proxy)):
        try:
            ip = proxy[i].strip().split("\t")
            print(ip)
            proxy_host = "http://"+ip[0]+":"+ip[1]
            proxy_temp = {"http":proxy_host}
            proxy_handler=urllib.request.ProxyHandler(proxy_temp)
            opener=urllib.request.build_opener(proxy_handler)
            res = opener.open(url).read()
            f.write(proxy[i]+'\n')
            print(proxy[i])
        except Exception as e:
            print(e)
            continue
    f.close()
if __name__ == '__main__':
    proxy = getProxyIp()
    validateIp(proxy)
```

> 运行结果如下：会把IP的一些情况打印出来，有些IP连接超时，有些拒绝访问，这些都是不可用的IP，将可用的IP写入到txt文件中。

![](https://i.imgur.com/v5BOlYg.png)

![](https://i.imgur.com/3EHgA5t.png)

#### 3.实例：使用IP代理爬取搜狗搜索网页测试

> 主要代码如下：

Weixin.py

```
# -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request
from items import WeixinItem
class WeixinSpider(scrapy.Spider):
    name = "WeiXin"
    allowed_domains = ["sogou.com"]
    start_urls = ['http://sogou.com/']

    def parse(self, response):
        '''建立循环页'''
        key='python'
        for i in range(1,11):
            url='http://weixin.sogou.com/weixin?query='+key+'&type=2&page='+str(i)
            yield Request(url=url,callback=self.get_content)
    def get_content(self,response):
        '''获取相关信息'''
        item=WeixinItem()
        item['title']=response.xpath('//div[@class="txt-box"]/h3/a').extract() #一页的全部标题,10条包含html标签
        item['link']=response.xpath('//div[@class="txt-box"]/h3/a/@href').extract() #一页的全部标题链接 10条
        item['dec']=response.xpath('//p[@class="txt-info"]').extract() #一页的全部描述,10条包含html标签
        yield item
```
items.py

```
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class WeixinItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title=scrapy.Field() #标题
    link=scrapy.Field() #链接
    dec=scrapy.Field() #描述

```
middlewares.py

```
# -*- coding: utf-8 -*-
import random
from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware #代理ip，这是固定的导入
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware #代理UA，固定导入
class IPPOOLS(HttpProxyMiddleware):
    def __init__(self,ip=''):
        '''初始化'''
        self.ip=ip
    def process_request(self, request, spider):
        '''使用代理ip，随机选用'''
        ip=random.choice(self.ip_pools) #随机选择一个ip
        print('当前使用的IP是'+ip['ip'])
        try:
            request.meta["proxy"]="http://"+ip['ip']
        except Exception as e:
            print(e)
            pass
    ip_pools=[
        {"ip":'222.42.227.81:80',
         },
        # {'ip':''},
    ]
class UAPOOLS(UserAgentMiddleware):
    def __init__(self,user_agent=''):
        self.user_agent=user_agent
    def process_request(self, request, spider):
        '''使用代理UA，随机选用'''
        ua=random.choice(self.user_agent_pools)
        print('当前使用的user-agent是'+ua)
        try:
            request.headers.setdefault('User-Agent',ua)
        except Exception as e:
            print(e)
            pass
    user_agent_pools=[
        'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3',
        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36',
    ]
```
pipelines.py

```
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html

import re
class WeixinPipeline(object):
    def process_item(self, item, spider):
        for i in range(len(item['title'])):
            html=item['title'][i]
            reg=re.compile(r'<[^>]+>') #去html标签
            title=reg.sub('',html)
            print(title)
            print (item['link'][i])
            html_1=item['dec'][i]
            reg_1=re.compile(r'<[^>]+>')
            dec=reg_1.sub('',html_1)
            print(dec)
        return item

```
> 运行结果如下：

![](https://i.imgur.com/kTn23cF.png)

![](https://i.imgur.com/XeCYKBa.png)

![](https://i.imgur.com/dUPVQ85.png)

![](https://i.imgur.com/ubBW4bL.png)

![](https://i.imgur.com/NHtWDq6.png)

#### 4.结论

目前我所使用的方法虽然能实现使用IP代理爬取网页的内容，但是经过测试发现这些免费代理网站的代理IP及其不稳定，有时候可能这一次可以爬取，下一次就连接不上或者连接超时。而且经过这个脚本检测过可以使用的IP经过测试也有好多不能使用的，所以目前我认为这种方法还不够稳定，还不足以可以使用在项目中，下一步我会去调研更多的方法来解决这个问题。
