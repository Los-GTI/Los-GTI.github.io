---
layout:     post
title:  Scrapy爬虫实例之三爬取鹅厂社招职位信息
subtitle: 爬虫实例三
date:       2018-01-26
author:     Los-GTI
header-img: img/scrapy11.jpg
catalog: true
tags:
    - Scrapy
---

#### 1.前言
本例主要是爬取鹅厂社会招聘的职位名、职位详情链接、职位类别、招聘人数、工作地点、发布时间等信息，此例也是简单的Scrapy爬虫实例，但是与爬取传智播客官网讲师信息那个例子稍有不同的是本次要爬取的信息有3000多条，而这三千多条信息又分页在300多个页面上，所以相比上次爬取传智播客官网讲师信息的例子，本例会增加一些额外的操作，下面让我们开始吧！
> 下图是我们要爬取的页面上的一些信息
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img/Scrapy例3_1.png)

#### 2.项目结构
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img/Scrapy例3_2.png)

#### 3.实现过程
##### 3.1 创建项目
使用命令创建名为tencent的工程：`scrapy startproject tecent`
##### 3.2 定义爬取目标
我们的目标是抓取鹅厂招聘的职位名、职位详情链接、职位类别、招聘人数、工作地点、发布信息等字段，确定了我们的抓取目标后，在items.py里定义，具体代码如下：
```
import scrapy
class TencentItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 职位名
    positionName = scrapy.Field()
    # 职位详情链接
    positionLink = scrapy.Field()
    # 职位类别
    positionType = scrapy.Field()
    # 招聘人数
    positionNumbers = scrapy.Field()
    # 工作地点
    positionLocation = scrapy.Field()
    # 发布时间
    publishTime = scrapy.Field()
```
##### 3.3 写爬虫
在写爬虫之前首先创建一个爬虫：`scrapy genspider tencentZP "tencent.com"`,此命令创建了一个名为tencentZP的爬虫，下面我们根据我们需要爬取的目标在tencentZP.py里面写爬虫，书写爬虫的过程主要是定义爬取的url和获取返回的item字段，然后将item字段交给管道处理，具体的代码如下：
```
# -*- coding: utf-8 -*-
import scrapy
from tencent.items import TencentItem

class TencentzpSpider(scrapy.Spider):
    name = 'tencentZP'
    allowed_domains = ['tencent.com']
    baseURL = "https://hr.tencent.com/position.php?&start="
    offset = 0
    start_urls = [baseURL + str(offset)]

    def parse(self, response):
        node_list = response.xpath("//tr[@class='even'] | //tr[@class='odd']")
        # 提取每个职位的信息，并且将提取到的Unicode数据转换为utf-8
        for node in node_list:
            item = TencentItem()
            item['positionName'] = node.xpath("./td[1]/a/text()").extract()[0]
            item['positionLink'] = node.xpath("./td[1]/a/@href").extract()[0]
            if len(node.xpath("./td[2]/text()")):
                item['positionType'] = node.xpath("./td[2]/text()").extract()[0]
            else:
                item['positionType'] = ""
            item['positionNumbers'] = node.xpath("./td[3]/text()").extract()[0]
            item['positionLocation'] = node.xpath("./td[4]/text()").extract()[0]
            item['publishTime'] = node.xpath("./td[5]/text()").extract()[0]
            yield item
        #if self.offset < 2190:
        #   self.offset += 10
        #   url = self.baseURL+str(self.offset)
        #   yield scrapy.Request(url,callback=self.parse)
        if len(response.xpath("//a[@class='noactive' and @id='next']"))==0:
            url = response.xpath("//a[@id='next']/@href").extract()[0]
            yield scrapy.Request("http://hr.tencent.com/" + url,callback=self.parse)
```
本例写爬虫的过程除了常规的利用xpath获取需要的返回值然后将数据存到item里面之外，你可以看到增加了最后几行代码，下面我就着重解释一下下面这几行代码：
```
if len(response.xpath("//a[@class='noactive' and @id='next']"))==0:
            url = response.xpath("//a[@id='next']/@href").extract()[0]
            yield scrapy.Request("http://hr.tencent.com/" + url,callback=self.parse)
```
这几行代码的作用是当爬虫爬取完一页的数据之后，我们需要获取下一页按钮对应的url，然后将此url再交给调度器去下载和返回数据，获取下一页对应的url还需要注意的是最后一页肯定是没有下一页的，所以我们需要做个判断，根据下一页按钮的class属性和id属性以及最后一页的下一页按钮的class属性和id属性，我们利用`len(response.xpath("//a[@class='noactive' and @id='next']"))==0`来判断不是最后一页，当它不是最后一页的时候我们才去获取到下一页按钮对应的url，然后将此url交给调度器处理。
##### 3.3 处理item数据
爬虫爬取到的数据会存到item字段中，然后爬虫会把item字段返回交给管道处理，所以我们需要在管道文件中处理返回的这些数据，处理这些数据主要是将这些数据转换为json格式然后写入到文件中，具体的代码如下（pipelines.py）：
```
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import json

class TencentPipeline(object):
    def __init__(self):
        self.f = open("tencent_abstract.json","w",encoding="utf-8")
    def process_item(self, item, spider):
        # dict(item)将item转换为python的s数据类型，返回的item为Scrapy的item字段
        # json.dumps(dict(item))再将pythomn的数据类型转换为json的格式
        content = json.dumps(dict(item),ensure_ascii=False) + ", \n"
        self.f.write(content)
        return item
    def close_spider(self):
        self.f.close()
```
##### 3.4 开启管道
在管道中写完处理数据的代码之后，我们需要在settings.py开启管道，将默认注释掉的管道文件配置重新启用，代码如下：
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img/Scrapy例3_3.png)
至此我们的爬虫文件，以及管道文件都书写完毕，上文中没有提到的其它文件均不需要修改，下面我们就运行这个爬虫。

#### 4.爬虫运行
在项目目录中使用命令`scrapy crawl tencentZP`运行名为tencentZP的爬虫，运行结果以及输出的json文件截图如下：
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img/Scrapy例3_4.png)
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img/Scrapy例3_5.png)

#### 5.总结
本次实例相比上一个爬取传智播客官网讲师信息的例子稍有不同，就是我们需要爬取不同页上的数据，所以我们在爬取完一页的时候还有返回给调度器下一页的url，然后调度器叫下载器去下载，然后返回数据，这样依次循环直到最后一页为止。

> 摁钉 author by Los-GTI

