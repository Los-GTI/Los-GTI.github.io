---
layout:     post
title:  Scrapy爬虫实例之二爬取传智播客官网讲师信息
subtitle: 爬虫实例
date:       2018-01-26
author:     Los-GTI
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Scrapy
---
#### 1.前言
本例是爬取传智播客官网讲师的姓名、职称、介绍等相关信息，爬取的网址为`http://www.itcast.cn/channel/teacher.shtml`

#### 2.项目结构
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img//Scrapy例2_1.png)

#### 3.实现过程
首先用命令创建一个Scrapy工程：`scrapy startproject itcast`
生成一个爬虫：`scrapy genspider ITcast itcast.cn`
生成爬虫之后抓取需要的数据，然后将数据交给管道处理，大致就是这样的过程，本例只修改了ITcast.py、items.py、pipelines.py、settings.py等文件，下面讲一下实现的具体过程。
##### 3.1 定义爬取目标
首先要确定爬取的目标以及要爬取什么信息，这里我要爬取的信息是传智播客官网上讲师介绍页面讲师的姓名、职称以及介绍等信息，定义爬取目标就在items.py里面来定义，代码如下（items.py）：
```
# -*- coding: utf-8 -*-
# Define here the models for your scraped items
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy
class ItcastItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()
    title = scrapy.Field()
    info = scrapy.Field()
```
##### 3.2 制作爬虫
首先生成一个爬虫：`scrapy genspider ITcast itcast.cn`

> ITcast是爬虫名，是爬虫的唯一标识。itcast.cn是爬取网页的域范围，表示爬虫爬取的网页范围不会超过这个域的范围

然后修改相关文件，根据自己需要爬取的内容自定义爬虫，我这边是爬取讲师的姓名、职称、介绍等相关信息、具体的代码如下（ITcast.py）:
```
# -*- coding: utf-8 -*-
import scrapy
from itcast.items import ItcastItem

class ItcastSpider(scrapy.Spider):
    #爬虫名为ITcast
    name = "ITcast"
    #allowed_domains=[]是搜索的域名范围，也就是爬虫的约束区域
    #规定爬虫只爬取这个域名下的网站，不存在的URL会被忽略
    allowed_domains = ["itcast.cn"]
    #start_urls=():爬取的URL元祖/列表，爬虫从这里开始抓取数据，所以
    #第一次下载的数据将会从这些urls开始，其他url将会从这些起始url继承
    start_urls = ["http://www.itcast.cn/channel/teacher.shtml"]

    #解析的方法，每个初始url完成下载后被调用，调用的时候传入从
    #每一个URL传回的Response对象作为唯一参数主要作用是：
    #负责解析返回的网页数据（response.body）,提取结构化数据（生成item）
    #生成需要下一页的URL请求
    def parse(self, response):
       for each in response.xpath("//div[@class='li_txt']"):
            #将我们得到的数据封装到一个ItcastItem对象
            item = ItcastItem()
            #extract()方法将xpath对象转换为unicode字符串
            name = each.xpath("./h3/text()").extract()
            title = each.xpath("./h4/text()").extract()
            info =  each.xpath("./p/text()").extract()

            #xpath返回的是包含一个元素的列表
            item['name'] = name[0]
            item['title'] = title[0]
            item['info'] = info[0]

            #将获取到的每个item数据都交给pipelines，同时还会回来继续执行
            #后面的代码，也就是for循环
            yield  item
```
##### 3.3处理数据
爬虫爬取的数据会交给管道来处理，我们在pipelines.py管道文件里来处理这些数据，我这边是将这些数据输出到一个json文件里面，代码如下（pipelines.py）：
```
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import json

class ItcastPipeline(object):
    #类初始化只会执行一次，所有item处理的数据都会存到一个json文件里
    def __init__(self):
        self.f=open("itcast_pipeline.json","wb+")
    def process_item(self, item, spider):
        content =json.dumps(dict(item),ensure_ascii=False) + ", \n"
        self.f.write(content.encode("utf-8"))
        return item
    def close_spider(self,spider):
        self.f.close()
```
##### 3.4设置文件
本例子中还修改了一丢丢配置文件，其实如果是单个管道出路爬取下来的数据的话这边不修改也无妨，主要修改的内容如下：
```
# 管道值越小优先级越高，如果有多个管道根据优先级依次通过多个管道
ITEM_PIPELINES = {
    'itcast.pipelines.ItcastPipeline': 300,
}
```
主要是为了区别如果定义多个管道的话，爬取下来的数据进入管道的优先级，后面的数值越小优先级越高，就会优先通过这个管道

##### 3.5运行
使用命令：`scrapy crawl ITcast`
运行截图以及生成的json文件截图如下：
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img//Scrapy例2_2.png)
![](https://raw.githubusercontent.com/Los-GTI/Los-GTI.github.io/master/img//Scrapy例2_3.png)

#### 4.总结
这个例子也是非常简单的例子，毕竟只是初学者，以后还会去做更多更复杂的例子，这边只是将学习的过程记录下来，能对一个人有用就心满意足了，有什么错误的地方欢迎指正。

> 摁钉 author by Los-GTI






